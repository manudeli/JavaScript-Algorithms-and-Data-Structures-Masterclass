Big O Notation
Way to formalize fuzzy counting
Allow us to talk formally about how the runtime of an algorithm grows as the inputs grow
We won't care about the details, only the trends

## Definition
We say that an algorithm is O(f(n)) if the
number of simple operations the
computer has to do is eventually less than
a constant times f(n), as n increases

- f(n) could be linear (f(n) = n)
- f(n) could be quadratic (f(n) = n**2)
- f(n) could be constant (f(n) = 1)
- f(n) could be something entirely different!


1. Time complexity

Simplifying Big O Expressions
When determining the time complexity of an algorithm,
there are som helpful rule of thumbs for big O expressions.
These rules of thumb are consequences of the definition of O notation

* Constants Don't Matter
O(2n) -> O(n)
O(500) -> O(1)
O(13n**2) -> O(n**2)

* Smaller Terms Don't Matter
O(n + 10) -> O(n)
O(100n + 50) -> O(n)
O(n**2 + 5n + 8) -> O(n**2)

* Big O Shorthands
1. Arithmetic operations are constant
2. Variable assignment is constant
3. Accessing elements in an array (by index) or object (by key) is constant
4. In a loop, the complexity is the length of the loop times the complexity of whatever happens inside of the loop


1. Space complexity
- Most primitives (booleans, numbers, undefined, null) are constant Space
- Strings require O(n) space (where n is the string length)
- Reference types are generally O(n), where n is the length (for arrays) or the number of keys (for objects)